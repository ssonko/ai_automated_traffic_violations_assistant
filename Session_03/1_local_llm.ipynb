{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d830bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#install': Expected package name at the start of dependency specifier\n",
      "    #install\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama #install in the .venv\n",
    "# !pip ollama #install on os\n",
    "# pip install ollama # runs .py files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2639cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from ollama) (2.12.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\alban ssonko\\training\\10_2025_gen_ai_training\\.venv\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.6.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install ollama\n",
    "import ollama\n",
    "\n",
    "def ask_question_local_llm(prompt):\n",
    "    print(f\"User asked: {prompt}\")\n",
    "\n",
    "    # Run a prompt against a local model (e.g., llama3)\n",
    "    response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assitant - Respond in one line\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a55df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: What is nice about ollama?\n",
      "Time taken by Local LLM: 9.751217126846313 seconds\n",
      "\n",
      "Local LLM says: Ollama is a popular board game that encourages social interaction, problem-solving, and storytelling, making it an enjoyable activity for friends and family to bond over.\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: Do you know Alban Ssonko?\n",
      "Time taken by Local LLM: 1.4933326244354248 seconds\n",
      "\n",
      "Local LLM says: Alban Sonko is a Gambian professional footballer who plays as a midfielder for the Gambia national team and several clubs, including St. Gallen FC and IF Elfsborg!\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: How about Arsene Wenger\n",
      "Time taken by Local LLM: 1.560455560684204 seconds\n",
      "\n",
      "Local LLM says: Ars√®ne Wenger is a French former footballer and manager who led Arsenal FC to three Premier League titles and seven FA Cups during his 22-year tenure as the team's manager.\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: How old is pytohon?\n",
      "Time taken by Local LLM: 10.392753601074219 seconds\n",
      "\n",
      "Local LLM says: Python was created in 1991 by Guido van Rossum, which makes it approximately 32 years old as of 2023.\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: \n",
      "Time taken by Local LLM: 8.689523220062256 seconds\n",
      "\n",
      "Local LLM says: I'm here to assist you with any questions or tasks you may have, so please feel free to ask me anything!\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: exit\n",
      "Time taken by Local LLM: 0.867023229598999 seconds\n",
      "\n",
      "Local LLM says: It was nice chatting with you, feel free to come back anytime for assistance!\n",
      "Exiting...\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: exit\n",
      "Time taken by Local LLM: 9.057274580001831 seconds\n",
      "\n",
      "Local LLM says: It seems you want to exit our conversation! It was nice chatting with you, feel free to reach out if you need any assistance in the future.\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: exit\n",
      "Time taken by Local LLM: 1.4537968635559082 seconds\n",
      "\n",
      "Local LLM says: It seems you've decided to exit the conversation! If you change your mind, feel free to come back and ask me anything. Have a great day!\n",
      "Exiting...\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: \n",
      "Time taken by Local LLM: 0.7358486652374268 seconds\n",
      "\n",
      "Local LLM says: How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "while True:\n",
    "    # Ask user for a question\n",
    "    user_prompt = input(\"Ask something: \")\n",
    "\n",
    "    if (user_prompt.lower() != 'quit'):\n",
    "\n",
    "        print (\"\\n\\n-------------------LOCAL LLM RESPONSE-------------------\")\n",
    "        start = time.time()\n",
    "        response_local = ask_question_local_llm(user_prompt)\n",
    "        end = time.time()   \n",
    "        print(f\"Time taken by Local LLM: {end - start} seconds\")\n",
    "        print(\"\\nLocal LLM says:\", response_local)        \n",
    "\n",
    "        # add delay of 3 seconds\n",
    "        time.sleep(3)\n",
    "    else:\n",
    "        print(\"Exiting...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
