{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3b6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def ask_question_local_llm(prompt):\n",
    "    print(f\"User asked: {prompt}\")\n",
    "    # my_client.chat.completions.create\n",
    "\n",
    "    # Run a prompt against a local model (e.g., llama2)\n",
    "    response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assitant - Respond in one line\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(response['message']['content'])\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2dcdaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "my_api_key = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "\n",
    "my_client = OpenAI(api_key=my_api_key)\n",
    "my_client\n",
    "\n",
    "def ask_question_open_ai(prompt):\n",
    "\n",
    "    print(f\"User asked: {prompt}\")\n",
    "    # my_client.chat.completions.create\n",
    "\n",
    "    llm_response = my_client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer as concisely as possible.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    print (llm_response)\n",
    "\n",
    "    return llm_response.choices[0].message.content  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b277266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------OPEN AI RESPONSE-------------------\n",
      "User asked: Why is the sun yellow?\n",
      "ChatCompletion(id='chatcmpl-CXF6kWMb4qqeMaMoucMdtbkIvU165', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Sun isn’t truly yellow. Its light is basically white (all colors mixed), but:\\n\\n- The Sun’s surface temperature makes its spectrum peak in the green region, and when all colors are combined it’s perceived as white.\\n- As sunlight passes through Earth’s atmosphere, shorter blue wavelengths scatter out (Rayleigh scattering). What reaches your eyes is the remaining, warmer light, which looks yellowish.\\n- Near sunrise/sunset the light travels through more atmosphere, scattering more blue and leaving red/orange tones.\\n\\nSo from space the Sun appears white; from the ground it often looks yellowish because of the atmosphere and how our eyes perceive color.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1762038934, model='gpt-5-nano-2025-08-07', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1227, prompt_tokens=30, total_tokens=1257, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=1088, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Time taken by OpenAI: 11.447421789169312 seconds\n",
      "\n",
      "OpenAI says: The Sun isn’t truly yellow. Its light is basically white (all colors mixed), but:\n",
      "\n",
      "- The Sun’s surface temperature makes its spectrum peak in the green region, and when all colors are combined it’s perceived as white.\n",
      "- As sunlight passes through Earth’s atmosphere, shorter blue wavelengths scatter out (Rayleigh scattering). What reaches your eyes is the remaining, warmer light, which looks yellowish.\n",
      "- Near sunrise/sunset the light travels through more atmosphere, scattering more blue and leaving red/orange tones.\n",
      "\n",
      "So from space the Sun appears white; from the ground it often looks yellowish because of the atmosphere and how our eyes perceive color.\n",
      "\n",
      "\n",
      "-------------------LOCAL LLM RESPONSE-------------------\n",
      "User asked: Why is the sun yellow?\n",
      "The sun appears yellow to us because our atmosphere scatters the shorter, blue wavelengths of light more than the longer, red and yellow wavelengths, giving it its characteristic yellowish hue.\n",
      "Time taken by Local LLM: 10.41365361213684 seconds\n",
      "\n",
      "Local LLM says: The sun appears yellow to us because our atmosphere scatters the shorter, blue wavelengths of light more than the longer, red and yellow wavelengths, giving it its characteristic yellowish hue.\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "while True:\n",
    "    # Ask user for a question\n",
    "    user_prompt = input(\"Ask something: \")\n",
    "\n",
    "    if (user_prompt.lower() != 'quit'):\n",
    "        # Get and print the response\n",
    "        print (\"-------------------OPEN AI RESPONSE-------------------\")\n",
    "        start = time.time()\n",
    "        response_openai = ask_question_open_ai(user_prompt)\n",
    "        end = time.time()   \n",
    "        print(f\"Time taken by OpenAI: {end - start} seconds\")\n",
    "        print(\"\\nOpenAI says:\", response_openai)\n",
    "\n",
    "        print (\"\\n\\n-------------------LOCAL LLM RESPONSE-------------------\")\n",
    "        start = time.time()\n",
    "        response_local = ask_question_local_llm(user_prompt)\n",
    "        end = time.time()   \n",
    "        print(f\"Time taken by Local LLM: {end - start} seconds\")\n",
    "        print(\"\\nLocal LLM says:\", response_local)        \n",
    "\n",
    "        # add delay of 3 seconds\n",
    "        time.sleep(3)\n",
    "    else:\n",
    "        print(\"Exiting...\")\n",
    "        break    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
